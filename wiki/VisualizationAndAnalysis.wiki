#summary Describes native tools to analyze and display the output

= Saving Snapshot Data as HDF5 =

The default way to extract data from the simulation is to dump it to an HDF5 file. To create just one use the function “write_snapshot_to_hdf5”. To create a series of snapshots at regular time intervals, use “ConsecutiveSnapshots” diagnostic. In case of parallel runs, each process writes to a different file. ConsecutiveSnapshots is already prepared for parallel simulations, but in order to use “write_snapshot_to_hdf5”, you would have to append the process rank to the file name, such that each process would write to a different file. 

= Structure of an HDF5 Snapshot =

The data inside an HDF5 file is in binary format, and is divided into different labelled bins. It can be read using the h5py python package. In what follows we explain what is stored in each of the different fields inside an HDF5 file


  * *time* Time of the simulation
  * *x_coordinate* - X coordinate of the mesh generating points
  * *y_coordinate* - Y coordinate of the mesh generating points
  * *density* - Density
  * *pressure* - Pressure
  * *x_velocity* - X component of the material velocity
  * *y_velocity* - Y component of the material velocity
  * *x position of vertices* X component of the vertices of the cells
  * *y position of vertices* Y component of the vertices of the cells
  * *Number of vertices in cell* Number of vertices around each mesh generating point
  * *Number of tracers* Number of passive tracers
  * *Tracer number #* Values of all tracers of index # (replace # with relevant tracer number)
  * *Cold Flow parameters* Values of the parameters a and b of the cold flows
  * *Cfl number* Value of the CFL number
  * *Cycle* number Number of times a time advance was called
  * *Density floor parameter* Density floor parameters
  * *Custom evolution indices* Indices of the custom evolution scheme

In parallel mode, the file also contains the following fields

  * *proc_x_coordinate* X coordinate of the process points
  * *proc_y_coordinate* Y coordinate of the process points

= Patch Plot =
In this kind of plot, each cell is added to the plot area and painted individually. The advantage in using such plot is that it shows the tessellation, and the disadvantage is that it is very slow. We have implemented scripts in both python and matlab to draw such plots.

== Python ==
The script can be found at “visualisation/two_dimensional/python/hdf5_voronoi_plot.py”. The syntax is
{{{
python hdf5_voronoi_plot.py [hdf5 file name] [name of hydrodynamic variable]
}}}
Since such plot takes a long time for a large number of points, we've only implemented it for serial simulations.

== Matlab == 
In the visualization directory there are matlab files to view the output. All of the files are documented. 

= Unstructured Contour =
This is another way to visualize the data. It does not show the tessellation, but it is much faster than the patch plot. We've implemented this method for both python and Matlab.

== Python ==
The python script for unstructured contour plots can be found at “visualisation/two_dimensional/python/multiplot.py”. The syntax is
{{{
python mutliplot.py [file pattern] [variable name]
}}}
where [file pattern] is a globbing pattern used to identify all the files with the snapshot data, and [variable name] is the name of the variable to be plotted (z axis).

In case of a serial run, suppose that the c++ line that created the snapshot was
{{{
write_snapshot_to_hdf5(sim,”snapshot.h5”);
}}}
then to display a contour map of the pressure, use
{{{
python multiplot.py snapshot.h5 pressure
}}}
where the Astrix {{{*}}} is a globbing wildcard.

Now, suppose for example that you would like to plot something that isn't covered by the existing tools. Let's say that you want to see a map of the entropy (assuming an ideal gas equation of state with adiabatic index 5/3). If the name of the snapshot file is "snapshot.h5", the appropriate python script would be
{{{
import pylab
import h5py
import numpy

h5_file_name = 'snapshot.h5'
g = 5./3. # Adiabatic index

f = h5py.File(h5_file_name)
pylab.tricontourf(f['x_coordinate'],
                  f['y_coordinate'],
                  numpy.log(f['pressure'])-g*numpy.log(f['density']))
f.close()
pylab.show()
}}}

== Function Fitting ==
This section will demonstrate how to use the lmfit python package to fit data to some multivariate function. In this example we start out with some 2D profile which we denote as "original" (in this case, it is a Gaussian). We then add random noise to obscure the original image. This profile we denote as "measured". Finally, we use function fitting to reconstruct the original image.
{{{
class MyFunc:
    
    def __init__(self, var_list):

        self.var_list = var_list

    def eval(self, x):

        import math

        mu_x = self.var_list['mu_x']
        mu_y = self.var_list['mu_y']
        sig_x = self.var_list['sig_x']
        sig_y = self.var_list['sig_y']

        return math.exp(-(x[0]-mu_x)**2/sig_x**2-(x[1]-mu_y)**2/sig_y)

def residual(params, x_list, data, eps_data):

    import numpy

    my_func = MyFunc(dict((key,params[key].value) 
                          for key in ['mu_x','mu_y','sig_x','sig_y']))

    model = numpy.array([my_func.eval(x) for x in x_list])

    return (data-model)/eps_data

def main():

    import numpy
    import pylab
    import random
    from lmfit import minimize, Parameters

    orig_func = MyFunc({'mu_x':1,'mu_y':2,'sig_x':3,'sig_y':4})

    coord_list = [[random.uniform(-10,10),random.uniform(-10,10)]
                   for i in range(10000)]
    z_exact = [orig_func.eval(coord) for coord in coord_list]
    z_measured = [z+random.gauss(0,0.1) for z in z_exact]

    params = Parameters()
    params.add('mu_x',value=5)
    params.add('mu_y',value=1)
    params.add('sig_x',value=1)
    params.add('sig_y',value=1)

    out = minimize(residual,params, 
                   args=(coord_list, z_measured,[1 for x in coord_list]))
    print out.values

    z_reconstructed = numpy.array([MyFunc(out.values).eval(point)
                                   for point in coord_list])

    z_data = {'Original':z_exact,
              'Measured':z_measured,
              'Reconstructed':z_reconstructed,
              'Difference':z_exact-z_reconstructed}
    for n, field in enumerate(['Original','Measured','Reconstructed','Difference']):
        pylab.subplot(2,2,n+1)
        pylab.title(field)
        pylab.tricontourf([itm[0] for itm in coord_list],
                          [itm[1] for itm in coord_list],
                          z_data[field])
        pylab.colorbar()
    pylab.show()

if __name__ == '__main__':

    main()
}}}
Here's an example of the output (actual results may change due to random numbers)

https://www.dropbox.com/s/qk4beaoncd9l374/lmfit_example.png?dl=0

== Matlab ==
TBA